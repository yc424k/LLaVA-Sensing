# LLaVA-Sensing 데이터 생성 시스템 회의록

**날짜**: 2025-01-10  
**참여자**: 사용자, Claude Code  
**주제**: 데이터 생성 시스템 이해 및 모델 변경

## 1. 데이터 생성 시스템 현황 파악

### 1.1 파일 생성 수량 확인
- **명령어**: `python3 preprocess_with_args.py --mode llm --max_files 2 --verbose --chunk_size 10`
- **결과**: 총 22개 파일 생성
  - 메인 데이터셋: `llm_dataset.json` (1개)
  - 청크 파일: `novel_dataset_chunk_001.json` ~ `novel_dataset_chunk_020.json` (20개)
  - 인덱스 파일: `chunks_index.json` (1개)
- **예제 수**: 총 200개 (max_files 2 × 100)

### 1.2 예제 생성 알고리즘 분석
**LLM 모드 처리 과정**:
1. **텍스트 청킹**: 소설을 문단 단위로 약 500자씩 분할
2. **환경 정보 분석**: 날씨, 시간, 장소, 감각적 표현 추출
3. **센서 데이터 생성**: 환경 정보 → 가상 센서 데이터 변환
4. **문학적 스타일 추론**: 텍스트 특성 분석하여 스타일 결정
5. **AI 텍스트 생성**: 센서 데이터 + 문학 스타일로 새 문단 생성

**Simple vs LLM 모드 차이점**:
- **Simple 모드**: chunk = 텍스트 처리 단위 (500자 조각)
- **LLM 모드**: chunk_size = 파일 분할 단위 (예제 개수)

## 2. 소설 데이터 현황

### 2.1 소설 파일 구조
```
/home/yc424k/LLaVA-Sensing/Novel/
├── Modernist_Novel/ (202개 파일)
└── Travel_Novel/ (201개 파일)
총: 403개 파일
```

### 2.2 코드 수정 사항
**목표**: 400개 파일에서 10,000개 예제를 균등하게 추출

**수정된 파일**:
1. `preprocess_with_args.py`:
   - `--num_examples` 매개변수 추가
   - LLM 모드에서 num_examples 우선 사용하도록 수정

2. `synthetic_dataset_generator.py`:
   - `generate_dataset()` 함수에 `max_files` 매개변수 추가
   - 파일당 예제 수 계산 로직 개선

## 3. AI 모델 변경 이력

### 3.1 사용된 모델들
**모델 변경 히스토리**:
1. **llama3.2:3b** (원래)
   - 크기: 2.0GB
   - 양자화: Q4_K_M (4-bit 주요, 6-bit/32-bit 혼합)
   
2. **deepseek-r1:8b**
   - 크기: 4.9GB
   - 다운로드 완료, 테스트 사용

3. **gpt-oss:20b**
   - 크기: 13GB
   - 다운로드 완료했으나 크기 문제로 사용 포기

4. **llama3.1:8b** (최종)
   - 크기: 4.7GB
   - 현재 설정된 모델

### 3.2 양자화 분석
**LLaMA 3.2 3B 양자화 상세**:
- 메인 양자화: Q4_K_M
- 혼합 정밀도: Q4_K (4-bit) + Q6_K (6-bit) + F32 (32-bit)
- 최적화: 성능과 효율성의 균형

## 4. 최종 설정 및 명령어

### 4.1 권장 LLM 모드 명령어
```bash
# 400개 파일에서 40,000개 예제 생성 (파일당 100개)
python3 preprocess_with_args.py --mode llm --max_files 400 --num_examples 40000 --verbose --chunk_size 100
```

### 4.2 Simple 모드 명령어 예시
```bash
# 기본
python3 preprocess_with_args.py --mode simple --max_files 100 --verbose

# 청크 크기 조정
python3 preprocess_with_args.py --mode simple --max_files 100 --verbose --chunk_size 50
```

## 5. 코드 개선 사항

### 5.1 구현된 개선점
1. **매개변수 분리**: num_examples와 max_files 독립적 설정 가능
2. **모델 히스토리 관리**: 주석으로 사용된 모델들 기록
3. **유연한 예제 분배**: 파일 수에 관계없이 원하는 예제 수 생성 가능

### 5.2 텍스트 청킹 예시
```
소설 파일 → "Extracted 77 text chunks"
→ 77개의 의미있는 텍스트 조각 생성
→ 신뢰도 0.3 이상인 것만 최종 예제로 선택
```

## 6. 향후 계획

### 6.1 추천 설정
- **모델**: llama3.1:8b (4.7GB, 성능/크기 균형)
- **데이터**: 400개 파일에서 40,000개 예제
- **청크**: 100개씩 분할하여 400개 파일 생성

### 6.2 모니터링 포인트
- 메모리 사용량 (llama3.1:8b 사용 시)
- 생성 품질 (환경 정보 추출 신뢰도)
- 처리 속도 (8B 모델의 inference 시간)

---

**회의 종료**: 모든 설정 완료, 데이터 생성 준비 완료